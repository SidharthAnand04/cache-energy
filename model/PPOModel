import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from datetime import datetime, timedelta
import requests
from statsmodels.tsa.arima.model import ARIMA
from sklearn.cluster import KMeans
from stable_baselines3 import PPO
from stable_baselines3.common.envs import DummyVecEnv
import gym
from gym import spaces

# Load electricity demand data from a CSV file
data = pd.read_csv("electricity_demand_data.csv")

# API to fetch energy price data
energy_price_api_url = ""

# Make an HTTP GET request to the energy price API
response_price = requests.get(energy_price_api_url)

if response_price.status_code == 200:
    data_price = response_price.json()

    # Extract energy price data from the API
    energy_price = np.array([entry['price']
                            for entry in data_price], dtype=float)
    data['energy_price'] = energy_price  # Add price data to the DataFrame

    # Define your custom charging environment
    class ChargingEnv(gym.Env):
        def __init__(self, data):
            super(ChargingEnv, self).__init__()

            # Data
            self.data = data
            self.current_step = 0
            self.max_steps = len(data)

            # Observation and action spaces
            self.observation_space = spaces.Box(
                low=0, high=1, shape=(3,), dtype=np.float32)
            self.action_space = spaces.Discrete(2)

        def reset(self):
            self.current_step = 0
            self.done = False
            self.total_reward = 0.0
            return self._next_observation()

        def step(self, action):
            if self.done:
                return None

            reward = self._take_action(action)
            self.total_reward += reward
            self.current_step += 1

            if self.current_step >= self.max_steps:
                self.done = True

            obs = self._next_observation()
            return obs, reward, self.done, {}

        def _take_action(self, action):
            if action == 1:  # Charge
                return self.data['energy_demand'].iloc[self.current_step] * self.data['energy_price'].iloc[self.current_step]
            else:  # Do not charge
                return 0

        def _next_observation(self):
            return np.array([
                self.data['time'].iloc[self.current_step] / self.max_steps,
                self.data['energy_demand'].iloc[self.current_step] /
                max(self.data['energy_demand']),
                self.data['energy_price'].iloc[self.current_step] /
                max(self.data['energy_price'])
            ])

    # Create a PPO environment
    env = ChargingEnv(data)
    env = DummyVecEnv([lambda: env])

    # Create and train a PPO model
    model = PPO("MlpPolicy", env, verbose=1)
    # You can adjust the number of timesteps
    model.learn(total_timesteps=50000)

    # Use the trained model to make future charging decisions
    forecast_steps = 24
    future_charging_decisions = []

    obs = env.reset()
    for _ in range(forecast_steps):
        action, _ = model.predict(obs)
        future_charging_decisions.append(action)
        obs, _, _, _ = env.step(action)

    # Save the future charging decisions to a CSV file
    future_data = pd.DataFrame({'Timestamp': data['time'].iloc[-1] + [timedelta(hours=i) for i in range(1, forecast_steps + 1)],
                                'ShouldCharge': future_charging_decisions})
    future_data.to_csv('future_charging_decisions.csv', index=False)

    # Visualization of data and optimal charge times
    plt.figure(figsize=(10, 6))
    plt.scatter(data['time'], data['energy_price'],
                c=future_charging_decisions, cmap='viridis')
    plt.xlabel('Timestamp')
    plt.ylabel('Price')
    plt.title('Electricity Price vs. Time with Future Charge Decisions')

    # Save the figure as a PNG file
    plt.savefig('future_charge_decision_plot.png')

    # Display the figure
    plt.show()

else:
    print("Failed to fetch data from the energy price API. Check the URL and status code.")
