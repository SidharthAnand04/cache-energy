import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from datetime import datetime, timedelta
import requests
import tensorflow as tf

# Load electricity demand data from a CSV file
data = pd.read_csv("electricity_demand_data.csv")

# API to fetch energy price data
energy_price_api_url = "https://hourlypricing.comed.com/api?type=5minutefeed"

# Make an HTTP GET request to the energy price API
response_price = requests.get(energy_price_api_url)

if response_price.status_code == 200:
    data_price = response_price.json()

    # Extract energy price data from the API
    energy_price = np.array([entry['price']
                            for entry in data_price], dtype=float)
    data['energy_price'] = energy_price  # Add price data to the DataFrame

    # Define your custom charging environment
    class ChargingEnv:
        def __init__(self, data):
            self.data = data
            self.current_step = 0
            self.max_steps = len(data)

        def reset(self):
            self.current_step = 0
            self._episode_ended = False
            return self._next_observation()

        def step(self, action):
            if self._episode_ended:
                return self.reset()

            reward = self._take_action(action)
            self.current_step += 1
            if self.current_step >= self.max_steps:
                self._episode_ended = True
                return self._next_observation(), reward, True, {}
            else:
                return self._next_observation(), reward, False, {}

        def _take_action(self, action):
            if action == 1:  # Charge
                return self.data['energy_demand'].iloc[self.current_step] * self.data['energy_price'].iloc[self.current_step]
            else:  # Do not charge
                return 0

        def _next_observation(self):
            return [
                self.data['time'].iloc[self.current_step] / self.max_steps,
                self.data['energy_demand'].iloc[self.current_step] /
                max(self.data['energy_demand']),
                self.data['energy_price'].iloc[self.current_step] /
                max(self.data['energy_price'])
            ]

    train_env = ChargingEnv(data)
    eval_env = ChargingEnv(data)

    # Define the actor and value network
    actor_net = tf.keras.Sequential([
        tf.keras.layers.Dense(64, activation='relu'),
        tf.keras.layers.Dense(2, activation=None),
    ])
    value_net = tf.keras.Sequential([
        tf.keras.layers.Dense(64, activation='relu'),
        tf.keras.layers.Dense(1, activation=None),
    ])

    # Define the optimizer
    optimizer = tf.keras.optimizers.Adam(learning_rate=1e-3)

    # Training loop
    num_episodes = 1000
    discount_factor = 0.99
    avg_return = []

    for episode in range(num_episodes):
        obs = train_env.reset()
        episode_return = 0

        while True:
            with tf.GradientTape() as tape:
                action_prob = actor_net(np.array([obs]))
                value = value_net(np.array([obs]))

                action = np.random.choice(2, p=action_prob[0])

                next_obs, reward, done, _ = train_env.step(action)

                td_error = reward + discount_factor * \
                    value_net(np.array([next_obs])) - value

                actor_loss = -tf.math.log(action_prob[0][action]) * td_error
                value_loss = td_error ** 2

                loss = actor_loss + value_loss

            grads = tape.gradient(
                loss, actor_net.trainable_variables + value_net.trainable_variables)
            optimizer.apply_gradients(
                zip(grads, actor_net.trainable_variables + value_net.trainable_variables))

            obs = next_obs
            episode_return += reward

            if done:
                break

        avg_return.append(episode_return)

        if episode % 100 == 0:
            print("Episode {}: Average Return: {}".format(
                episode, np.mean(avg_return[-100:])))

    # Save the future charging decisions to a CSV file
    forecast_steps = 24
    future_charging_decisions = []

    obs = eval_env.reset()
    for _ in range(forecast_steps):
        action_prob = actor_net(np.array([obs]))
        action = np.random.choice(2, p=action_prob[0])
        future_charging_decisions.append(action)
        obs, _, _, _ = eval_env.step(action)

    future_data = pd.DataFrame({'Timestamp': data['time'].iloc[-1] + [timedelta(hours=i) for i in range(1, forecast_steps + 1)],
                                'ShouldCharge': future_charging_decisions})
    future_data.to_csv('future_charging_decisions.csv', index=False)

    # Visualization of data and optimal charge times
    plt.figure(figsize=(10, 6))
    plt.scatter(data['time'], data['energy_price'],
                c=future_charging_decisions, cmap='viridis')
    plt.xlabel('Timestamp')
    plt.ylabel('Price')
    plt.title('Electricity Price vs. Time with Future Charge Decisions')

    # Save the figure as a PNG file
    plt.savefig('future_charge_decision_plot.png')

    # Display the figure
    plt.show()

else:
    print("Failed to fetch data from the energy price API. Check the URL and status code.")
