import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from datetime import datetime, timedelta
import requests
import torch
import torch.nn as nn
import torch.optim as optim

# Load electricity demand data from a CSV file
data = pd.read_csv("electricity_demand_data.csv")

# Convert the 'time' column to datetime objects
data['time'] = pd.to_datetime(data['time'])

# API to fetch energy price data
energy_price_api_url = "https://hourlypricing.comed.com/api?type=5minutefeed"

# Make an HTTP GET request to the energy price API
response_price = requests.get(energy_price_api_url)

if response_price.status_code == 200:
    data_price = response_price.json()

    # Extract energy price data from the API
    energy_price = np.array([entry['price']
                            for entry in data_price], dtype=float)

    # Resample 'energy_price' data to match the time intervals in the DataFrame
    data_time_numeric = (data['time'] - data['time'].min()).dt.total_seconds()
    data_time_numeric = np.linspace(
        data_time_numeric.min(), data_time_numeric.max(), len(energy_price))

    # Check if the lengths match
    if len(data) == len(energy_price):
        data['energy_price'] = energy_price  # Add price data to the DataFrame
    else:
        print("Length mismatch between data and energy_price. Please check your data.")

    # Define your custom charging environment

    class ChargingEnv:
        def __init__(self, data):
            self.data = data
            self.current_step = 0
            self.max_steps = len(data)

        def reset(self):
            self.current_step = 0
            self._episode_ended = False
            return self._next_observation()

        def step(self, action):
            if self._episode_ended:
                return self.reset()

            reward = self._take_action(action)
            self.current_step += 1
            if self.current_step >= self.max_steps:
                self._episode_ended = True
                return self._next_observation(), reward, True, {}
            else:
                return self._next_observation(), reward, False, {}

        def _take_action(self, action):
            if action == 1:  # Charge
                return self.data['energy_demand'].iloc[self.current_step] * self.data['energy_price'].iloc[self.current_step]
            else:  # Do not charge
                return 0

        def _next_observation(self):
            return [
                self.data_time_numeric[self.current_step] / self.max_steps,
                self.data['energy_demand'].iloc[self.current_step] /
                max(self.data['energy_demand']),
                self.data['energy_price'].iloc[self.current_step] /
                max(self.data['energy_price'])
            ]

    train_env = ChargingEnv(data)
    eval_env = ChargingEnv(data)

    # Define the actor and value network using PyTorch
    class ActorNet(nn.Module):
        def __init__(self):
            super(ActorNet, self).__init__()
            self.fc1 = nn.Linear(3, 64)
            self.fc2 = nn.Linear(64, 2)

        def forward(self, x):
            x = torch.relu(self.fc1(x))
            x = self.fc2(x)
            return x

    class ValueNet(nn.Module):
        def __init__(self):
            super(ValueNet, self).__init()
            self.fc1 = nn.Linear(3, 64)
            self.fc2 = nn.Linear(64, 1)

        def forward(self, x):
            x = torch.relu(self.fc1(x))
            x = self.fc2(x)
            return x

    actor_net = ActorNet()
    value_net = ValueNet()

    # Define the optimizer
    optimizer = optim.Adam(list(actor_net.parameters()) +
                           list(value_net.parameters()), lr=1e-3)

    # Training loop
    num_episodes = 1000
    discount_factor = 0.99
    avg_return = []

    for episode in range(num_episodes):
        obs = train_env.reset()
        episode_return = 0

        while True:
            action_prob = torch.softmax(
                actor_net(torch.tensor([obs], dtype=torch.float32), dim=-1))
            value = value_net(torch.tensor([obs], dtype=torch.float32))

            action = torch.multinomial(action_prob, 1).item()

            next_obs, reward, done, _ = train_env.step(action)

            td_error = reward + discount_factor * \
                value_net(torch.tensor(
                    [next_obs], dtype=torch.float32)) - value

            actor_loss = -torch.log(action_prob[0][action]) * td_error
            value_loss = td_error ** 2

            loss = actor_loss + value_loss

            optimizer.zero_grad()
            loss.backward()
            optimizer.step()

            obs = next_obs
            episode_return += reward

            if done:
                break

        avg_return.append(episode_return)

        if episode % 100 == 0:
            print("Episode {}: Average Return: {}".format(
                episode, np.mean(avg_return[-100:])))

    # Save the future charging decisions to a CSV file
    forecast_steps = 24
    future_charging_decisions = []

    obs = eval_env.reset()
    for _ in range(forecast_steps):
        action_prob = torch.softmax(
            actor_net(torch.tensor([obs], dtype=torch.float32), dim=-1))
        action = torch.multinomial(action_prob, 1).item()
        future_charging_decisions.append(action)
        obs, _, _, _ = eval_env.step(action)

    future_data = pd.DataFrame({'Timestamp': data['time'].iloc[-1] + [timedelta(hours=i) for i in range(1, forecast_steps + 1)],
                                'ShouldCharge': future_charging_decisions})
    future_data.to_csv('future_charging_decisions.csv', index=False)

    # Visualization of data and optimal charge times
    plt.figure(figsize=(10, 6))
    plt.scatter(data['time'], data['energy_price'],
                c=future_charging_decisions, cmap='viridis')
    plt.xlabel('Timestamp')
    plt.ylabel('Price')
    plt.title('Electricity Price vs. Time with Future Charge Decisions')

    # Save the figure as a PNG file
    plt.savefig('future_charge_decision_plot.png')

    # Display the figure
    plt.show()

else:
    print("Failed to fetch data from the energy price API. Check the URL and status code.")
